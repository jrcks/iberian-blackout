{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bae57b8e23c77b0",
   "metadata": {},
   "source": [
    "# Iberian outage analysis\n",
    "\n",
    "We have access to daily data for 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888717f3fe793105",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:07:16.983752Z",
     "start_time": "2025-05-16T08:07:16.926682Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gzip\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ipaddress\n",
    "\n",
    "import boto3\n",
    "from botocore.utils import fix_s3_host\n",
    "from botocore.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca783cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load secret environment variables\n",
    "with open(\".env\") as f:\n",
    "    for line in f:\n",
    "        if line.strip() == '' or line.strip().startswith('#'):\n",
    "            continue\n",
    "        key, value = line.strip().split('=', 1)\n",
    "        os.environ[key] = value\n",
    "\n",
    "# Set up global constants\n",
    "DATA_DIR = \"data/\"\n",
    "ROWS = 1_000_000\n",
    "\n",
    "# The date of the data we are using\n",
    "YEAR = 2025\n",
    "MONTH = 5\n",
    "DAY = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7d4848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boto3 resource using environment variables\n",
    "S3 = boto3.resource(\n",
    "    's3',\n",
    "    aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
    "    aws_secret_access_key=os.environ['AWS_ACCESS_KEY_SECRET'],\n",
    "    endpoint_url=os.environ['AWS_ENDPOINT_URL'],\n",
    "    # Change timeouts in case we are uploading large files\n",
    "    config=Config(\n",
    "        connect_timeout=3, \n",
    "        read_timeout=900, \n",
    "        retries={\"max_attempts\":0}\n",
    "    )\n",
    ")\n",
    "\n",
    "# Unregister to ensure requests donâ€™t go to AWS\n",
    "S3.meta.client.meta.events.unregister('before-sign.s3', fix_s3_host)\n",
    "\n",
    "# Use bucket name from environment\n",
    "HOME_BUCKET = S3.Bucket(os.environ['AWS_BUCKET_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3886512a74ee2cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:07:21.570989Z",
     "start_time": "2025-05-16T08:07:21.564307Z"
    }
   },
   "outputs": [],
   "source": [
    "# MAnycastR files have metadata that give information about the measurement\n",
    "def read_gzipped_comment_lines(filepath, comment_char='#'):\n",
    "    \"\"\"Read initial comment lines from a gzipped file.\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File not found: '{filepath}'\")\n",
    "\n",
    "    try:\n",
    "        with gzip.open(filepath, 'rt', encoding='utf-8') as f:\n",
    "            comment_lines = []\n",
    "            # Read lines until the first non-comment line\n",
    "            for line in f:\n",
    "                if line.startswith(comment_char):\n",
    "                    comment_lines.append(line.rstrip())\n",
    "                else:\n",
    "                    # Stop at first non-comment line\n",
    "                    break\n",
    "            return comment_lines\n",
    "\n",
    "    except gzip.BadGzipFile:\n",
    "        raise gzip.BadGzipFile(f\"Invalid gzip file: '{filepath}'\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error reading file '{filepath}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4dcec090dc5467",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:07:22.226791Z",
     "start_time": "2025-05-16T08:07:22.220080Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to create a hostname mapping (Client ID -> hostname)\n",
    "def create_hostname_mapping(comment_lines):\n",
    "    \"\"\"Map Client ID to hostname from comment lines.\"\"\"\n",
    "    pattern = r\"ID:\\s*(\\d+)\\s*,\\s*hostname:\\s*([\\w-]+)\"\n",
    "    mapping = {}\n",
    "\n",
    "    for line in comment_lines:\n",
    "        if (match := re.search(pattern, line)):\n",
    "            client_id = int(match.group(1))       # Extract Client ID\n",
    "            hostname = match.group(2)             # Extract hostname\n",
    "            mapping[client_id] = hostname\n",
    "\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6826fbce3302ed2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:16:27.145564Z",
     "start_time": "2025-05-16T08:16:27.139848Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_manycast_file(year, month, day, anycast=True, ipv6=False):\n",
    "    \"\"\"Download a (Manycast/Unicast, IPv4/IPv6) file from MinIO if not already present.\"\"\"\n",
    "\n",
    "    # Build object prefix based on date\n",
    "    prefix = f\"manycast/{year}/{month:02}/{day:02}/\"\n",
    "\n",
    "    # Choose file pattern based on anycast and IP version\n",
    "    protocol = \"ICMPv6\" if ipv6 else \"ICMPv4\"\n",
    "    base_pattern = f\"MAnycast_{protocol}\" if anycast else f\"GCD_{protocol}\"\n",
    "\n",
    "    # Search for matching file in bucket\n",
    "    for obj in HOME_BUCKET.objects.filter(Prefix=prefix):\n",
    "         # Replace invalid Windows characters in filenames\n",
    "        filename = re.sub(r'[:<>\"/\\\\|?*]', '_', obj.key[len(prefix):])\n",
    "        filepath = os.path.join(DATA_DIR, filename)\n",
    "\n",
    "        if filename.startswith(base_pattern) and filename.endswith('.csv.gz'):\n",
    "            print(f\"Found file: {filename} (bucket key: {obj.key})\")\n",
    "\n",
    "            # Check if file already exists locally\n",
    "            if os.path.exists(filepath):\n",
    "                print(f\"File {filename} already exists. Skipping download.\")\n",
    "            else:\n",
    "                print(f\"Downloading {filename} from bucket...\")\n",
    "                os.makedirs(DATA_DIR, exist_ok=True)\n",
    "                HOME_BUCKET.download_file(obj.key, filepath)\n",
    "\n",
    "            return filepath\n",
    "\n",
    "    print(\"No matching file found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca152d2da13105fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:19:19.853107Z",
     "start_time": "2025-05-16T08:16:31.949823Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download the file for the specified date\n",
    "filepath = get_manycast_file(YEAR, MONTH, DAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d701f00e5baedea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:19:25.702473Z",
     "start_time": "2025-05-16T08:19:25.695681Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get metadata\n",
    "comment_lines = read_gzipped_comment_lines(filepath)\n",
    "print(\"\\n\".join(comment_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd45ef10e10863a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:19:30.326275Z",
     "start_time": "2025-05-16T08:19:30.319848Z"
    }
   },
   "outputs": [],
   "source": [
    "# create hostname mapping\n",
    "hostname_map = create_hostname_mapping(comment_lines)\n",
    "hostname_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37eb8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data as a pandas dataframe\n",
    "result_df = pd.read_csv(filepath, skiprows=len(comment_lines), nrows=ROWS, compression='gzip')\n",
    "result_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211bed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning as we are only using the following columns:\n",
    "result_df = result_df[['rx_worker_id', 'tx_worker_id', 'reply_src_addr', 'rx_time', 'tx_time', 'ttl']]\n",
    "result_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bfee5566d09679",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:22:02.163186Z",
     "start_time": "2025-05-16T08:20:54.980130Z"
    }
   },
   "outputs": [],
   "source": [
    "# we scan an address in each /24 prefix, which is representative of that prefix\n",
    "# each target is scanned by all 32 anycast sites\n",
    "# in your case this might give redundancy, so make sure to take into consideration that you will see the same prefix multiple times (e.g., remove duplicate targets from the analysis)\n",
    "\n",
    "# convert IP-number to ip network\n",
    "result_df['target'] = result_df['reply_src_addr'].apply(\n",
    "    # can be sped up with swifter\n",
    "    lambda x: ipaddress.ip_network(f\"{ipaddress.ip_address(x)}/24\", strict=False)\n",
    ")\n",
    "\n",
    "# get receiving anycast site\n",
    "result_df['receiver'] = result_df['rx_worker_id'].map(hostname_map)\n",
    "\n",
    "# get sending anycast site\n",
    "result_df['sender'] = result_df['tx_worker_id'].map(hostname_map)\n",
    "\n",
    "# calculate rtt\n",
    "result_df['rtt'] = ((result_df['rx_time'] - result_df['tx_time']) / 1e6)\n",
    "\n",
    "# drop unnecessary columns\n",
    "result_df = result_df[['receiver', 'sender', 'target', 'rtt', 'ttl']]\n",
    "\n",
    "result_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c2d4837c5acfe6",
   "metadata": {},
   "source": [
    "**Some ideas:**\n",
    "- what is the impact on average hop count (TTL)?\n",
    "- what is the impact on average RTT?\n",
    "- which prefixes became unreachable?\n",
    "- which prefixes shifted catchment?\n",
    "- where did the prefixes that switched catchment go?\n",
    "\n",
    "Also consider looking at GCD and filtering on sender == madrid -> how many prefixes are reachable from there?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb4dacc",
   "metadata": {},
   "source": [
    "# Work for project\n",
    "## geolocate ip addresses\n",
    "We may not be able to map all addresses, due to IPs missing in the geolocation database.\n",
    "Also the IP may be missing or wrongly geolocated, due to changes over time (however the prefixes generally should stay in a certain region (maybe find a source for this claim))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d675ce",
   "metadata": {},
   "source": [
    "### initial set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e135496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from typing import Generator\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gzip\n",
    "import re\n",
    "import ipaddress\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import boto3\n",
    "from botocore.utils import fix_s3_host\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import EndpointConnectionError\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6379ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "# Load secret environment variables\n",
    "with open(\".env\") as f:\n",
    "    for line in f:\n",
    "        if line.strip() == '' or line.strip().startswith('#'):\n",
    "            continue\n",
    "        key, value = line.strip().split('=', 1)\n",
    "        os.environ[key] = value\n",
    "\n",
    "# Set up global constants for paths\n",
    "DATA_DIR = Path(\"data/\")\n",
    "IP_GEOLOCATION_DB_PATH = DATA_DIR / \"IP2LOCATION-LITE-DB5.CSV\"\n",
    "IP_GEOLOCATION_DOWNLOAD_PATH = DATA_DIR / \"ip2location-lite-db5.zip\"\n",
    "IP_GEOLOCATION_URL = f\"https://www.ip2location.com/download/?token={os.environ[\"IP2LOCATION_LITE_TOKEN\"]}&file=DB5LITECSV\"\n",
    "# MANYCAST_DATA_PATH = DATA_DIR / \"MAnycast_ICMPv42025-05-01T01_37_56.csv.gz\"\n",
    "\n",
    "ROWS = 1_000_000\n",
    "\n",
    "# The date of the data we are using\n",
    "YEAR = 2025\n",
    "MONTH = 5\n",
    "DAY = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba94e8b",
   "metadata": {},
   "source": [
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0a53d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bucket():\n",
    "    \"\"\"\n",
    "    Creates a bucket based on provided environment variables \n",
    "\n",
    "    Make sure to set AWS_ACCESS_KEY_ID, AWS_ACCESS_KEY_SECRET, AWS_ENDPOINT_URL, AWS_BUCKET_NAME\n",
    "    before creating the bucket\n",
    "    \"\"\"\n",
    "        # Create boto3 resource using environment variables\n",
    "    S3 = boto3.resource(\n",
    "        's3',\n",
    "        aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
    "        aws_secret_access_key=os.environ['AWS_ACCESS_KEY_SECRET'],\n",
    "        endpoint_url=os.environ['AWS_ENDPOINT_URL'],\n",
    "        # Change timeouts in case we are uploading large files\n",
    "        config=Config(\n",
    "            connect_timeout=3, \n",
    "            read_timeout=900, \n",
    "            retries={\"max_attempts\":0}\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Unregister to ensure requests donâ€™t go to AWS\n",
    "    S3.meta.client.meta.events.unregister('before-sign.s3', fix_s3_host)\n",
    "\n",
    "    # Use bucket name from environment\n",
    "    return S3.Bucket(os.environ['AWS_BUCKET_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9fad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_manycast_files(bucket, data_dir: Path, year=2025, month=5, day=1, anycast=True, ipv6=False) -> Generator:\n",
    "    \"\"\"Download a (Manycast/Unicast, IPv4/IPv6) file from MinIO if not already present.\"\"\"\n",
    "    # Build object prefix based on date\n",
    "    prefix = f\"manycast/{year}/{month:02}/{day:02}/\"\n",
    "\n",
    "    # Choose file pattern based on anycast and IP version\n",
    "    protocol = \"ICMPv6\" if ipv6 else \"ICMPv4\"\n",
    "    base_pattern = f\"MAnycast_{protocol}\" if anycast else f\"GCD_{protocol}\"\n",
    "\n",
    "    # Search for matching file in bucket\n",
    "    try:\n",
    "        for obj in bucket.objects.filter(Prefix=prefix):\n",
    "            # Replace invalid Windows characters in filenames\n",
    "            filename = re.sub(r'[:<>\"/\\\\|?*]', '_', obj.key[len(prefix):])\n",
    "            filepath = os.path.join(data_dir, filename)\n",
    "\n",
    "            if filename.startswith(base_pattern) and filename.endswith('.csv.gz'):\n",
    "                if DEBUG:\n",
    "                    print(f\"Found file: {filename} (bucket key: {obj.key})\")\n",
    "                # Check if file already exists locally\n",
    "                if os.path.exists(filepath):\n",
    "                    if DEBUG:\n",
    "                        print(f\"File {filename} already exists. Skipping download.\")\n",
    "                else:\n",
    "                    if DEBUG:\n",
    "                        print(f\"Downloading {filename} from bucket...\")\n",
    "                    os.makedirs(data_dir, exist_ok=True)\n",
    "                    bucket.download_file(obj.key, filepath)\n",
    "\n",
    "                yield filepath\n",
    "    except EndpointConnectionError:\n",
    "        print(\"Could not access the Bucket, falling back to local data.\")\n",
    "        for file in os.listdir(data_dir):\n",
    "            if file.startswith(base_pattern + f\"{year}-{month:02}-{day:02}\"):\n",
    "                yield data_dir / file \n",
    "\n",
    "    print(\"No matching file found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd0b538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_ip2location_db(ip_geoloc_db: Path, ip_geoloc_dl_path: Path, ip_geoloc_dl_url: str):\n",
    "    # retrieve ip2location database\n",
    "    if not ip_geoloc_db.exists():\n",
    "        # download\n",
    "        response = requests.get(ip_geoloc_dl_url, allow_redirects=True)\n",
    "        if not response.ok:\n",
    "            print(\"Download of IP2Location database failed!\")\n",
    "            return     \n",
    "        with open(ip_geoloc_dl_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        # extract\n",
    "        with zipfile.ZipFile(ip_geoloc_dl_path, \"r\") as zipf:\n",
    "            zipf.extractall(\"data\")\n",
    "        \n",
    "        # remove downloaded zip file\n",
    "        os.remove(ip_geoloc_dl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa70726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create a hostname mapping (Client ID -> hostname)\n",
    "def create_hostname_mapping(comment_lines):\n",
    "    \"\"\"Map Client ID to hostname from comment lines.\"\"\"\n",
    "    pattern = r\"ID:\\s*(\\d+)\\s*,\\s*hostname:\\s*([\\w-]+)\"\n",
    "    mapping = {}\n",
    "\n",
    "    for line in comment_lines:\n",
    "        if (match := re.search(pattern, line)):\n",
    "            client_id = int(match.group(1))       # Extract Client ID\n",
    "            hostname = match.group(2)             # Extract hostname\n",
    "            mapping[client_id] = hostname\n",
    "\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c6eb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAnycastR files have metadata that give information about the measurement\n",
    "def read_gzipped_comment_lines(filepath, comment_char='#'):\n",
    "    \"\"\"Read initial comment lines from a gzipped file.\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File not found: '{filepath}'\")\n",
    "\n",
    "    try:\n",
    "        with gzip.open(filepath, 'rt', encoding='utf-8') as f:\n",
    "            comment_lines = []\n",
    "            # Read lines until the first non-comment line\n",
    "            for line in f:\n",
    "                if line.startswith(comment_char):\n",
    "                    comment_lines.append(line.rstrip())\n",
    "                else:\n",
    "                    # Stop at first non-comment line\n",
    "                    break\n",
    "            return comment_lines\n",
    "\n",
    "    except gzip.BadGzipFile:\n",
    "        raise gzip.BadGzipFile(f\"Invalid gzip file: '{filepath}'\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error reading file '{filepath}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1fd41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_manycast_data(manycast_data_path: Path) -> (pd.DataFrame | str):\n",
    "    \"\"\"\n",
    "    Process a MAnycast csv.gz file\n",
    "\n",
    "    Returns:     \n",
    "        Tuple[pd.DataFrame, str]: A resulting dataframe containing the data and the metadata as a string\n",
    "    \"\"\"\n",
    "    # Preprocessing of MAnycast data\n",
    "    comment_lines = read_gzipped_comment_lines(manycast_data_path)\n",
    "    hostname_map = create_hostname_mapping(comment_lines)\n",
    "\n",
    "    # load in data as a pandas dataframe\n",
    "    manycast_df = pd.read_csv(manycast_data_path, skiprows=len(comment_lines), nrows=ROWS, compression='gzip')\n",
    "\n",
    "    # reduce to relevant columns\n",
    "    manycast_df = manycast_df[['rx_worker_id', 'tx_worker_id', 'reply_src_addr', 'rx_time', 'tx_time', 'ttl']]\n",
    "\n",
    "    # convert IP-number to ip network\n",
    "    manycast_df['target'] = manycast_df['reply_src_addr'].apply(\n",
    "        # can be sped up with swifter => uv pip install swifter \"swifter[groupby] swifter[notebook]\"\n",
    "        lambda x: ipaddress.ip_network(f\"{ipaddress.ip_address(x)}/24\", strict=False)\n",
    "    )\n",
    "\n",
    "    # get receiving anycast site\n",
    "    manycast_df['receiver'] = manycast_df['rx_worker_id'].map(hostname_map)\n",
    "    # get sending anycast site\n",
    "    manycast_df['sender'] = manycast_df['tx_worker_id'].map(hostname_map)\n",
    "    # calculate rtt\n",
    "    manycast_df['rtt'] = ((manycast_df['rx_time'] - manycast_df['tx_time']) / 1e6)\n",
    "\n",
    "    # drop unnecessary columns\n",
    "    manycast_df = manycast_df[['receiver', 'sender', 'target', 'reply_src_addr', 'rtt', 'ttl']]\n",
    "    manycast_df = manycast_df.rename(columns={\"reply_src_addr\": \"encoded_target_addr\"})\n",
    "\n",
    "    return manycast_df, comment_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac985121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_ip2location(ip_geoloc_db: Path) -> pd.DataFrame:\n",
    "    # read the ip2location database\n",
    "    return pd.read_csv(ip_geoloc_db,\n",
    "                       names=[\"ip_from\", \"ip_to\", \"country_code\", \"country_name\", \"region\", \"city\", \"lat\", \"lon\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd703440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_manycast_geolocated(manycast_df: pd.DataFrame, ip_geoloc_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns: the manycast_df, with added lat and lon values corresponding to encoded_target_addr\n",
    "    \"\"\"\n",
    "    sorted_ip_geoloc_df = ip_geoloc_df.sort_values(\"ip_from\")\n",
    "    sorted_manycast_df = manycast_df.sort_values(\"encoded_target_addr\")\n",
    "\n",
    "    # merge_asof for efficient range join\n",
    "    merged = pd.merge_asof(\n",
    "        sorted_manycast_df,\n",
    "        sorted_ip_geoloc_df,\n",
    "        left_on=\"encoded_target_addr\",\n",
    "        right_on=\"ip_from\",\n",
    "        direction=\"backward\"\n",
    "    )\n",
    "\n",
    "    # filter entry when reply_src_addr > ip_to\n",
    "    merged = merged[merged[\"encoded_target_addr\"] <= merged[\"ip_to\"]]\n",
    "\n",
    "    sorted_manycast_df.insert(len(sorted_manycast_df.columns), \"lat\", 0.0)\n",
    "    sorted_manycast_df.insert(len(sorted_manycast_df.columns), \"lon\", 0.0)\n",
    "\n",
    "    sorted_manycast_df[\"lat\"] = merged[\"lat\"].values\n",
    "    sorted_manycast_df[\"lon\"] = merged[\"lon\"].values\n",
    "\n",
    "    return sorted_manycast_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9bab74",
   "metadata": {},
   "source": [
    "### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456c5a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve manycast data\n",
    "bucket = get_bucket()\n",
    "manycast_file = next(dl_manycast_files(bucket, DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0e508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "manycast_df, meta_inf = preproc_manycast_data(manycast_file)\n",
    "if DEBUG:\n",
    "    print(meta_inf)\n",
    "    print(manycast_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0242ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip2geoloc_df = preproc_ip2location(IP_GEOLOCATION_DB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d8c3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    # check amount of invalid lat long values for any entries in the ip2location database (first + 1, last - 1)\n",
    "    lat0_lon0_amount = ((ip2geoloc_df[\"lon\"] == 0.0) & (ip2geoloc_df[\"lat\"] == 0.0)).sum()\n",
    "    print(\"invalid lat long values found in ip2location database: \", lat0_lon0_amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ec89e9",
   "metadata": {},
   "source": [
    "### add latitude and longitude to the manycast data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7da7d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "manycast_df = get_manycast_geolocated(manycast_df, ip2geoloc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7458a595",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(manycast_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
