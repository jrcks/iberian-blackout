{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bae57b8e23c77b0",
   "metadata": {},
   "source": [
    "# Iberian outage analysis\n",
    "\n",
    "We have access to daily data for 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888717f3fe793105",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:07:16.983752Z",
     "start_time": "2025-05-16T08:07:16.926682Z"
    }
   },
   "outputs": [],
   "source": [
    "%pip install boto3\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import gzip\n",
    "import ipaddress\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import boto3\n",
    "from botocore.utils import fix_s3_host\n",
    "from botocore.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20612408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load secret environment variables\n",
    "with open(\".env\") as f:\n",
    "    for line in f:\n",
    "        if line.strip() == '' or line.strip().startswith('#'):\n",
    "            continue\n",
    "        key, value = line.strip().split('=', 1)\n",
    "        os.environ[key] = value\n",
    "\n",
    "# Set up global constants\n",
    "DATA_DIR = \"data/\"\n",
    "\n",
    "# Protocol and IP version constants\n",
    "PROTOCOL = \"ICMP\"  # Currently only ICMP is supported\n",
    "IP_VERSION = 4     # 4 or 6\n",
    "ANYCAST = True     # Anycast or Unicast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6944c2836231af8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:07:17.901498Z",
     "start_time": "2025-05-16T08:07:17.659357Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create boto3 resource using environment variables\n",
    "S3 = boto3.resource(\n",
    "    's3',\n",
    "    aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
    "    aws_secret_access_key=os.environ['AWS_ACCESS_KEY_SECRET'],\n",
    "    endpoint_url=os.environ['AWS_ENDPOINT_URL'],\n",
    "    # Change timeouts in case we are uploading large files\n",
    "    config=Config(\n",
    "        connect_timeout=3, \n",
    "        read_timeout=900, \n",
    "        retries={\"max_attempts\":0}\n",
    "    )\n",
    ")\n",
    "\n",
    "# Unregister to ensure requests don’t go to AWS\n",
    "S3.meta.client.meta.events.unregister('before-sign.s3', fix_s3_host) # type: ignore[attr-defined]\n",
    "\n",
    "# Use bucket name from environment\n",
    "HOME_BUCKET = S3.Bucket(os.environ['AWS_BUCKET_NAME']) # type: ignore[attr-defined]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca152d2da13105fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:19:19.853107Z",
     "start_time": "2025-05-16T08:16:31.949823Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_minio_file(\n",
    "    year: int, month: int, day: int,\n",
    "    anycast: bool, protocol: str, ip_version: int\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Download a (Manycast/Unicast, IPv4/IPv6) CSV file from MinIO if not already present locally.\n",
    "    Prints the result.\n",
    "    \"\"\"\n",
    "    prefix = f\"manycast/{year}/{month:02}/{day:02}/\"\n",
    "    proto = f\"{protocol}v{ip_version}\"\n",
    "    base_pattern = f\"MAnycast_{proto}\" if anycast else f\"GCD_{proto}\"\n",
    "\n",
    "    for obj in HOME_BUCKET.objects.filter(Prefix=prefix):\n",
    "        filename = obj.key[len(prefix):]\n",
    "        # Clean up filename for Windows compatibility\n",
    "        filename = re.sub(r'[:<>\"/\\\\|?*]', '_', filename)\n",
    "        filepath = os.path.join(DATA_DIR, filename)\n",
    "\n",
    "        # Only try files matching the pattern and proper extension\n",
    "        if filename.startswith(base_pattern) and filename.endswith('.csv.gz'):\n",
    "            print(f\"Found file: {filename} (bucket key: {obj.key})\")\n",
    "\n",
    "            # Check if the file already exists locally\n",
    "            if os.path.exists(filepath):\n",
    "                print(f\"File {filename} already exists locally. Skipping download.\")\n",
    "            else:\n",
    "                print(f\"Downloading {filename} from bucket...\")\n",
    "                os.makedirs(DATA_DIR, exist_ok=True)\n",
    "                t0 = time.time()\n",
    "                HOME_BUCKET.download_file(obj.key, filepath)\n",
    "                t1 = time.time()\n",
    "                print(f\"Download complete in {t1-t0:.2f}s\")\n",
    "\n",
    "            return filepath\n",
    "\n",
    "    # If no matching file is found, print a message\n",
    "    print(\"No matching file found in bucket for provided criteria.\")\n",
    "    raise FileNotFoundError(\n",
    "        f\"No file found for {base_pattern} on {year}-{month:02}-{day:02} in bucket {HOME_BUCKET.name}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d701f00e5baedea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:19:25.702473Z",
     "start_time": "2025-05-16T08:19:25.695681Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_comments(filepath: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Prints the leading comment lines (lines starting with '#') from a gzip-compressed CSV file.\n",
    "    Returns the comment lines as a list.\n",
    "    \"\"\"\n",
    "    # Raise error if file is missing\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File not found: '{filepath}'\")\n",
    "\n",
    "    try:\n",
    "        # Open the gzip file in text mode for reading\n",
    "        with gzip.open(filepath, 'rt', encoding='utf-8') as f:\n",
    "            # Collect all leading lines that start with '#'\n",
    "            comment_lines = []\n",
    "            for line in f:\n",
    "                if line.startswith('#'):\n",
    "                    comment_lines.append(line.rstrip())\n",
    "                else:\n",
    "                    # Stop at the first data (non-comment) line\n",
    "                    return comment_lines\n",
    "\n",
    "    except gzip.BadGzipFile:\n",
    "        raise gzip.BadGzipFile(f\"Invalid gzip file: '{filepath}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error reading file '{filepath}': {e}\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd45ef10e10863a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:19:30.326275Z",
     "start_time": "2025-05-16T08:19:30.319848Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_hostname_map(comment_lines: list[str]) -> dict[int, str]:\n",
    "    \"\"\"\n",
    "    Create a mapping of Client ID to hostname from comment lines.\n",
    "    \"\"\"\n",
    "    pattern = r\"ID:\\s*(\\d+)\\s*,\\s*hostname:\\s*([\\w-]+)\"\n",
    "    hostname_map = {}\n",
    "\n",
    "    for line in comment_lines:\n",
    "        # Look for a line containing 'ID' and 'hostname'\n",
    "        if match := re.search(pattern, line):\n",
    "            client_id = int(match.group(1))      # Extract and convert ID\n",
    "            hostname = match.group(2)            # Extract hostname\n",
    "            hostname_map[client_id] = hostname   # Fill mapping\n",
    "\n",
    "    # Return the mapping sorted by worker id\n",
    "    return dict(sorted(hostname_map.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb1bf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(\n",
    "    chunk: pd.DataFrame,\n",
    "    hostname_map: dict[int, str],\n",
    "    tx_worker_id: int | None = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter by tx_worker_id and process a DataFrame chunk to add hostname.\n",
    "    \"\"\"\n",
    "    # Avoid SettingWithCopyWarning\n",
    "    chunk = chunk.copy()\n",
    "\n",
    "    # Filter rows on tx_worker_id if supplied\n",
    "    if tx_worker_id is not None:\n",
    "        chunk = chunk[chunk['tx_worker_id'] == tx_worker_id]\n",
    "\n",
    "    # Map hostnames for sender and receiver\n",
    "    chunk['receiver'] = chunk['rx_worker_id'].map(hostname_map)\n",
    "    chunk['sender'] = chunk['tx_worker_id'].map(hostname_map)\n",
    "\n",
    "    # Convert IP-number to ip network /24\n",
    "    chunk['target'] = chunk['reply_src_addr'].apply(\n",
    "        lambda x: ipaddress.ip_network(f\"{ipaddress.ip_address(int(x))}/24\", strict=False) # type: ignore\n",
    "    )\n",
    "\n",
    "    # Calculate RTT in seconds\n",
    "    chunk['rtt'] = (chunk['rx_time'] - chunk['tx_time']) / 1e6\n",
    "\n",
    "    # Return only the needed columns\n",
    "    return chunk[['receiver', 'sender', 'target', 'rtt', 'ttl']]\n",
    "\n",
    "def csv_to_df(\n",
    "    filepath: str,\n",
    "    hostname_map: dict[int, str],\n",
    "    chunksize: int = 1_000_000,\n",
    "    tx_worker: str | None = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a large, gzipped CSV file in chunks and filter rows by a given tx_worker_id.\n",
    "    Prints progress and a summary upon completion.\n",
    "\n",
    "    Returns:\n",
    "        (DataFrame, read_time_sec, process_time_sec)\n",
    "    \"\"\"\n",
    "    filtered_chunks = []\n",
    "    chunk_num: int = 0\n",
    "\n",
    "    # Get tx_worker_id if tx_worker name is supplied\n",
    "    tx_worker_id = None\n",
    "    if tx_worker is not None:\n",
    "        try:\n",
    "            tx_worker_id = next(k for k, v in hostname_map.items() if v.startswith(tx_worker))\n",
    "        except StopIteration:\n",
    "            raise ValueError(f\"No tx_worker_id found for tx_worker name starting with: {tx_worker}\")\n",
    "\n",
    "    # Read the CSV file in chunks\n",
    "    print(f\"Reading file: {filepath} in chunks of {chunksize:,} rows...\")\n",
    "    t0 = time.time()\n",
    "    chunks = pd.read_csv(\n",
    "        filepath,\n",
    "        compression='gzip',\n",
    "        comment='#',\n",
    "        usecols=['rx_worker_id', 'tx_worker_id', 'reply_src_addr', 'rx_time', 'tx_time', 'ttl'],\n",
    "        dtype={\n",
    "            'rx_worker_id': 'uint8',\n",
    "            'tx_worker_id': 'uint8',\n",
    "            'reply_src_addr': 'uint32' if IP_VERSION == 4 else 'str', # For IPv4 use unit32, IPv6 uses str\n",
    "            'rx_time': 'float64',\n",
    "            'tx_time': 'float64',\n",
    "            'ttl': 'uint8'\n",
    "        },\n",
    "        chunksize=chunksize\n",
    "    )\n",
    "\n",
    "    # Process each chunk\n",
    "    t1 = time.time()\n",
    "    for chunk in chunks:\n",
    "        filtered_chunk = process_chunk(chunk, hostname_map, tx_worker_id)\n",
    "        filtered_chunks.append(filtered_chunk)\n",
    "        chunk_num += 1\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Read {chunk_num} chunks\", end='\\r')\n",
    "\n",
    "    # Processing complete\n",
    "    t2 = time.time()\n",
    "    print(f\"\\nProcessing complete! Time taken: {t1 - t0:.2f}s (reading) + {t2 - t1:.2f}s (processing)\")\n",
    "    print(f\"Processed {sum(len(c) for c in filtered_chunks):,} entries!\")\n",
    "\n",
    "    return pd.concat(filtered_chunks, ignore_index=True)\n",
    "\n",
    "def load(\n",
    "    year: int, month: int, day: int,\n",
    "    anycast: bool, protocol: str, ip_version: int\n",
    ") -> pd.DataFrame:\n",
    "    # Download the file from MinIO if it exists\n",
    "    filepath = download_minio_file(\n",
    "        year, month, day,\n",
    "        anycast, protocol, ip_version\n",
    "    )\n",
    "    \n",
    "    # Get all comment lines found\n",
    "    comment_lines = extract_comments(filepath)\n",
    "    \n",
    "    # Extract the hostname map from the comment lines\n",
    "    hostname_map = extract_hostname_map(comment_lines)\n",
    "    \n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = csv_to_df(filepath, hostname_map, tx_worker='de-fra')\n",
    "\n",
    "    # Drop the duplicates based on receiver, sender, and target\n",
    "    df.drop_duplicates(subset=['receiver', 'sender', 'target'], keep='first', inplace=True)\n",
    "\n",
    "    # Return the loaded dataframe\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c2d4837c5acfe6",
   "metadata": {},
   "source": [
    "**Some ideas:**\n",
    "- what is the impact on average hop count (TTL)?\n",
    "- what is the impact on average RTT?\n",
    "- which prefixes became unreachable?\n",
    "- which prefixes shifted catchment?\n",
    "- where did the prefixes that switched catchment go?\n",
    "\n",
    "Also consider looking at GCD and filtering on `sender == madrid` -> how many prefixes are reachable from there?\n",
    "\n",
    "**Tasks:**\n",
    "- the ’normal’ situation before the outage, showing which hosts (in what parts of the world) routed to Madrid\n",
    "- the number, and locations, of hosts that went unresponsive during the outage\n",
    "- hosts that shifted routing from Madrid to a different anycast site (using e.g., a Sankey diagram), e.g., how many hosts shifted to Paris? How many hosts shifted to Frankfurt? etc..\n",
    "- the situation after the outage, did routing return to normal? or are changes still visible?\n",
    "- overall reachability of the Madrid site (using the unicast data) towards the entire IPv4 Internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f07b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# The date of the data we are using\n",
    "YEAR = 2025\n",
    "MONTH = 4\n",
    "DAY = 28\n",
    "\n",
    "# Load the data\n",
    "df = load(YEAR, MONTH, DAY, ANYCAST, PROTOCOL, IP_VERSION)\n",
    "\n",
    "# Print the first 10 rows\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
