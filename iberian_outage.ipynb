{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bae57b8e23c77b0",
   "metadata": {},
   "source": [
    "# Iberian outage analysis\n",
    "\n",
    "We have access to daily data for 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888717f3fe793105",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:07:16.983752Z",
     "start_time": "2025-05-16T08:07:16.926682Z"
    }
   },
   "outputs": [],
   "source": [
    "%pip install boto3\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import gzip\n",
    "import ipaddress\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import boto3\n",
    "from botocore.utils import fix_s3_host\n",
    "from botocore.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20612408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load secret environment variables\n",
    "with open(\".env\") as f:\n",
    "    for line in f:\n",
    "        if line.strip() == '' or line.strip().startswith('#'):\n",
    "            continue\n",
    "        key, value = line.strip().split('=', 1)\n",
    "        os.environ[key] = value\n",
    "\n",
    "# Set up global constants\n",
    "DATA_DIR = \"data/\"\n",
    "\n",
    "# Protocol and IP version constants\n",
    "PROTOCOL = \"ICMP\"  # Currently only ICMP is supported\n",
    "IP_VERSION = 6     # 4 or 6\n",
    "ANYCAST = True     # Anycast or Unicast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6944c2836231af8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:07:17.901498Z",
     "start_time": "2025-05-16T08:07:17.659357Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create boto3 resource using environment variables\n",
    "S3 = boto3.resource(\n",
    "    's3',\n",
    "    aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
    "    aws_secret_access_key=os.environ['AWS_ACCESS_KEY_SECRET'],\n",
    "    endpoint_url=os.environ['AWS_ENDPOINT_URL'],\n",
    "    # Change timeouts in case we are uploading large files\n",
    "    config=Config(\n",
    "        connect_timeout=3, \n",
    "        read_timeout=900, \n",
    "        retries={\"max_attempts\":0}\n",
    "    )\n",
    ")\n",
    "\n",
    "# Unregister to ensure requests donâ€™t go to AWS\n",
    "S3.meta.client.meta.events.unregister('before-sign.s3', fix_s3_host) # type: ignore[attr-defined]\n",
    "\n",
    "# Use bucket name from environment\n",
    "HOME_BUCKET = S3.Bucket(os.environ['AWS_BUCKET_NAME']) # type: ignore[attr-defined]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca152d2da13105fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:19:19.853107Z",
     "start_time": "2025-05-16T08:16:31.949823Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_minio_file(\n",
    "    year: int, month: int, day: int,\n",
    "    anycast: bool, protocol: str, ip_version: int\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Download a (Manycast/Unicast, IPv4/IPv6) CSV file from MinIO if not already present locally.\n",
    "    Prints the result.\n",
    "    \"\"\"\n",
    "    prefix = f\"manycast/{year}/{month:02}/{day:02}/\"\n",
    "    proto = f\"{protocol}v{ip_version}\"\n",
    "    base_pattern = f\"MAnycast_{proto}\" if anycast else f\"GCD_{proto}\"\n",
    "\n",
    "    for obj in HOME_BUCKET.objects.filter(Prefix=prefix):\n",
    "        filename = obj.key[len(prefix):]\n",
    "        # Clean up filename for Windows compatibility\n",
    "        filename = re.sub(r'[:<>\"/\\\\|?*]', '_', filename)\n",
    "        filepath = os.path.join(DATA_DIR, filename)\n",
    "\n",
    "        # Only try files matching the pattern and proper extension\n",
    "        if filename.startswith(base_pattern) and filename.endswith('.csv.gz'):\n",
    "            print(f\"Found file: {filename} (bucket key: {obj.key})\")\n",
    "\n",
    "            # Check if the file already exists locally\n",
    "            if os.path.exists(filepath):\n",
    "                print(f\"File {filename} already exists locally. Skipping download.\")\n",
    "            else:\n",
    "                print(f\"Downloading {filename} from bucket...\")\n",
    "                os.makedirs(DATA_DIR, exist_ok=True)\n",
    "                t0 = time.time()\n",
    "                HOME_BUCKET.download_file(obj.key, filepath)\n",
    "                t1 = time.time()\n",
    "                print(f\"Download complete in {t1-t0:.2f}s\")\n",
    "\n",
    "            return filepath\n",
    "\n",
    "    # If no matching file is found, print a message\n",
    "    print(\"No matching file found in bucket for provided criteria.\")\n",
    "    raise FileNotFoundError(\n",
    "        f\"No file found for {base_pattern} on {year}-{month:02}-{day:02} in bucket {HOME_BUCKET.name}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d701f00e5baedea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:19:25.702473Z",
     "start_time": "2025-05-16T08:19:25.695681Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_comments(filepath: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Prints the leading comment lines (lines starting with '#') from a gzip-compressed CSV file.\n",
    "    Returns the comment lines as a list.\n",
    "    \"\"\"\n",
    "    # Raise error if file is missing\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File not found: '{filepath}'\")\n",
    "\n",
    "    try:\n",
    "        # Open the gzip file in text mode for reading\n",
    "        with gzip.open(filepath, 'rt', encoding='utf-8') as f:\n",
    "            # Collect all leading lines that start with '#'\n",
    "            comment_lines = []\n",
    "            for line in f:\n",
    "                if line.startswith('#'):\n",
    "                    comment_lines.append(line.rstrip())\n",
    "                else:\n",
    "                    # Stop at the first data (non-comment) line\n",
    "                    return comment_lines\n",
    "\n",
    "    except gzip.BadGzipFile:\n",
    "        raise gzip.BadGzipFile(f\"Invalid gzip file: '{filepath}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error reading file '{filepath}': {e}\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd45ef10e10863a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:19:30.326275Z",
     "start_time": "2025-05-16T08:19:30.319848Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_hostname_map(comment_lines: list[str]) -> dict[int, str]:\n",
    "    \"\"\"\n",
    "    Create a mapping of Client ID to hostname from comment lines.\n",
    "    \"\"\"\n",
    "    pattern = r\"ID:\\s*(\\d+)\\s*,\\s*hostname:\\s*([\\w-]+)\"\n",
    "    hostname_map = {}\n",
    "\n",
    "    for line in comment_lines:\n",
    "        # Look for a line containing 'ID' and 'hostname'\n",
    "        if match := re.search(pattern, line):\n",
    "            client_id = int(match.group(1))      # Extract and convert ID\n",
    "            hostname = match.group(2)            # Extract hostname\n",
    "            hostname_map[client_id] = hostname   # Fill mapping\n",
    "\n",
    "    # Return the mapping sorted by worker id\n",
    "    return dict(sorted(hostname_map.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb1bf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(\n",
    "    chunk: pd.DataFrame,\n",
    "    hostname_map: dict[int, str],\n",
    "    tx_worker_id: int | None = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter by tx_worker_id and process a DataFrame chunk to add hostname.\n",
    "    \"\"\"\n",
    "    # Avoid SettingWithCopyWarning\n",
    "    chunk = chunk.copy()\n",
    "\n",
    "    # Filter rows on tx_worker_id if supplied\n",
    "    if tx_worker_id is not None:\n",
    "        chunk = chunk[chunk['tx_worker_id'] == tx_worker_id]\n",
    "\n",
    "    # Map hostnames for sender and receiver\n",
    "    chunk['receiver'] = chunk['rx_worker_id'].map(hostname_map)\n",
    "    chunk['sender'] = chunk['tx_worker_id'].map(hostname_map)\n",
    "\n",
    "    # Convert IP-number to ip network /24\n",
    "    chunk['target'] = chunk['reply_src_addr'].apply(\n",
    "        lambda x: ipaddress.ip_network(f\"{ipaddress.ip_address(int(x))}/24\", strict=False) # type: ignore\n",
    "    )\n",
    "\n",
    "    # Calculate RTT in seconds\n",
    "    chunk['rtt'] = (chunk['rx_time'] - chunk['tx_time']) / 1e6\n",
    "\n",
    "    # Return only the needed columns\n",
    "    return chunk[['receiver', 'sender', 'target', 'rtt', 'ttl']]\n",
    "\n",
    "def csv_to_df(\n",
    "    filepath: str,\n",
    "    hostname_map: dict[int, str],\n",
    "    chunksize: int = 1_000_000,\n",
    "    tx_worker: str | None = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a large, gzipped CSV file in chunks and filter rows by a given tx_worker_id.\n",
    "    Prints progress and a summary upon completion.\n",
    "\n",
    "    Returns:\n",
    "        (DataFrame, read_time_sec, process_time_sec)\n",
    "    \"\"\"\n",
    "    filtered_chunks = []\n",
    "    chunk_num: int = 0\n",
    "\n",
    "    # Get tx_worker_id if tx_worker name is supplied\n",
    "    tx_worker_id = None\n",
    "    if tx_worker is not None:\n",
    "        try:\n",
    "            tx_worker_id = next(k for k, v in hostname_map.items() if v.startswith(tx_worker))\n",
    "        except StopIteration:\n",
    "            raise ValueError(f\"No tx_worker_id found for tx_worker name starting with: {tx_worker}\")\n",
    "\n",
    "    # Read the CSV file in chunks\n",
    "    print(f\"Reading file: {filepath} in chunks of {chunksize:,} rows...\")\n",
    "    t0 = time.time()\n",
    "    chunks = pd.read_csv(\n",
    "        filepath,\n",
    "        compression='gzip',\n",
    "        comment='#',\n",
    "        usecols=['rx_worker_id', 'tx_worker_id', 'reply_src_addr', 'rx_time', 'tx_time', 'ttl'],\n",
    "        dtype={\n",
    "            'rx_worker_id': 'uint8',\n",
    "            'tx_worker_id': 'uint8',\n",
    "            'reply_src_addr': 'uint32' if IP_VERSION == 4 else 'str', # For IPv4 use unit32, IPv6 uses str\n",
    "            'rx_time': 'float64',\n",
    "            'tx_time': 'float64',\n",
    "            'ttl': 'uint8'\n",
    "        },\n",
    "        chunksize=chunksize\n",
    "    )\n",
    "\n",
    "    # Process each chunk\n",
    "    t1 = time.time()\n",
    "    for chunk in chunks:\n",
    "        filtered_chunk = process_chunk(chunk, hostname_map, tx_worker_id)\n",
    "        filtered_chunks.append(filtered_chunk)\n",
    "        chunk_num += 1\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Read {chunk_num} chunks\", end='\\r')\n",
    "\n",
    "    # Processing complete\n",
    "    t2 = time.time()\n",
    "    print(f\"\\nProcessing complete! Time taken: {t1 - t0:.2f}s (reading) + {t2 - t1:.2f}s (processing)\")\n",
    "    print(f\"Processed {sum(len(c) for c in filtered_chunks):,} entries!\")\n",
    "\n",
    "    return pd.concat(filtered_chunks, ignore_index=True)\n",
    "\n",
    "def load(\n",
    "    year: int, month: int, day: int,\n",
    "    anycast: bool, protocol: str, ip_version: int\n",
    ") -> pd.DataFrame:\n",
    "    # Download the file from MinIO if it exists\n",
    "    filepath = download_minio_file(\n",
    "        year, month, day,\n",
    "        anycast, protocol, ip_version\n",
    "    )\n",
    "    \n",
    "    # Get all comment lines found\n",
    "    comment_lines = extract_comments(filepath)\n",
    "    \n",
    "    # Extract the hostname map from the comment lines\n",
    "    hostname_map = extract_hostname_map(comment_lines)\n",
    "    \n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = csv_to_df(filepath, hostname_map, tx_worker='de-fra')\n",
    "\n",
    "    # Drop the duplicates based on receiver, sender, and target\n",
    "    df.drop_duplicates(subset=['receiver', 'sender', 'target'], keep='first', inplace=True)\n",
    "\n",
    "    # Return the loaded dataframe\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c2d4837c5acfe6",
   "metadata": {},
   "source": [
    "**Some ideas:**\n",
    "- what is the impact on average hop count (TTL)?\n",
    "- what is the impact on average RTT?\n",
    "- which prefixes became unreachable?\n",
    "- which prefixes shifted catchment?\n",
    "- where did the prefixes that switched catchment go?\n",
    "\n",
    "Also consider looking at GCD and filtering on sender == madrid -> how many prefixes are reachable from there?\n",
    "\n",
    "**Tasks:**\n",
    "- the â€™normalâ€™ situation before the outage, showing which hosts (in what parts of the world) routed to Madrid\n",
    "- the number, and locations, of hosts that went unresponsive during the outage\n",
    "- hosts that shifted routing from Madrid to a different anycast site (using e.g., a Sankey diagram), e.g., how many hosts shifted to Paris? How many hosts shifted to Frankfurt? etc..\n",
    "- the situation after the outage, did routing return to normal? or are changes still visible?\n",
    "- overall reachability of the Madrid site (using the unicast data) towards the entire IPv4 Internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f07b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# The date of the data we are using\n",
    "YEAR = 2025\n",
    "MONTH = 4\n",
    "DAY = 28\n",
    "\n",
    "# Load the data\n",
    "df = load(YEAR, MONTH, DAY, ANYCAST, PROTOCOL, IP_VERSION)\n",
    "\n",
    "# Print the first 10 rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93421d41",
   "metadata": {},
   "source": [
    "# Sankey diagram of the distribution (Task C)\n",
    "\n",
    "This code generates a Sankey diagram visualizing the distribution of host routing changes. \n",
    "Specifically, it shows how many hosts previously routing through Madrid have shifted their routing to other anycast sites. \n",
    "The diagram illustrates the number of hosts that migrated to each destination anycast location (e.g., Paris, Frankfurt, etc.) after the routing change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6176ec5e-8c0f-44f2-a914-fc138f8fc92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.colors as plc\n",
    "from collections import defaultdict\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# Function to load from an date range\n",
    "def load_range(from_date, to_date, *load_args, step=1):\n",
    "    out = []\n",
    "    current = from_date\n",
    "    while current <= to_date:\n",
    "        df = load(current.year, current.month, current.day, *load_args)\n",
    "        label = current.strftime('%d-%m')\n",
    "        out.append((df, label))\n",
    "        current += timedelta(days=step)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abba20f-ab23-448e-970c-37ee8ef608f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "FROM = date(2025, 4, 28)\n",
    "TO = date(2025, 5, 2)\n",
    "dfs_and_labels = load_range(FROM, TO, ANYCAST, PROTOCOL, IP_VERSION, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d7b8c2-8635-45c2-8bf1-347f6bfdf549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING\n",
    "\n",
    "# 0. Unpack DataFrames and labels\n",
    "print(\"Unpacking DataFrames and labels...\")\n",
    "dfs, labels = zip(*dfs_and_labels)\n",
    "\n",
    "\n",
    "# 1. Collect all targets that ever targeted the specified target\n",
    "target = 'es-mad-manycast'\n",
    "targets_series_list = []\n",
    "print(f\"Aggregating targets that send to '{target}' from all DataFrames...\")\n",
    "for i, df in enumerate(dfs):\n",
    "    print(f\"  Scanning DataFrame {i+1}/{len(dfs)} for targets targeting '{target}'...\")\n",
    "    if {'target', 'receiver'}.issubset(df.columns):\n",
    "        targets_series_list.append(df.loc[df['receiver'] == target, 'target'])\n",
    "\n",
    "print(\"  Concatenating all target series and extracting unique targets...\")\n",
    "targets = set(pd.concat(targets_series_list, ignore_index=False).unique())\n",
    "print(f\"  Found {len(targets)} unique '{target}'\")\n",
    "\n",
    "\n",
    "# 2. Prepare receiver column names\n",
    "print(\"Generating receiver column names for each label...\")\n",
    "receiver_cols = [f\"receiver_{label}\" for label in labels]\n",
    "\n",
    "\n",
    "# 3. Filter and rename the columns\n",
    "print(\"Filtering and renaming DataFrames...\")\n",
    "dfs_indexed = []\n",
    "for i, (df, receiver_col) in enumerate(zip(dfs, receiver_cols)):\n",
    "    print(f\"  Processing DataFrame {i+1}/{len(dfs)}: filtering rows and renaming column...\")\n",
    "    # Filter to madrid_targets and select ['target', 'receiver']\n",
    "    filtered = df.loc[df['target'].isin(targets), ['target', 'receiver']]\n",
    "    # Rename 'receiver' to unique column for this stage\n",
    "    renamed = filtered.rename(columns={'receiver': receiver_col})\n",
    "    # Drop duplicate targets, keep first occurrence only\n",
    "    deduped = renamed.drop_duplicates('target', keep='first')\n",
    "    # Set index for fast merging\n",
    "    dfs_indexed.append(deduped.set_index('target'))\n",
    "print(\"  All DataFrames filtered and columns renamed\")\n",
    "\n",
    "\n",
    "# 4. Merge all at once via concat/outer join\n",
    "print(\"Merging all DataFrames on 'target' (outer join)...\")\n",
    "df_seq = pd.concat(dfs_indexed, axis=1, sort=False).reset_index()\n",
    "df_seq = df_seq[['target'] + receiver_cols]\n",
    "print(\"Merge completed. Final DataFrame shape:\", df_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02752e3-99e7-4aaf-99d4-5b27a06324cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 400\n",
    "n_stages = len(receiver_cols)\n",
    "\n",
    "# 1. Fast: Count receiver occurrences per stage (incl. unresponsive)\n",
    "print(\"1. Counting receiver occurrences per stage (incl. unresponsive)...\")\n",
    "stage_receiver_counts = []\n",
    "for stage_idx, receiver_col in enumerate(receiver_cols):\n",
    "    counts = df_seq[receiver_col].fillna('unresponsive').astype(str).value_counts().to_dict()\n",
    "    stage_receiver_counts.append(defaultdict(int, counts))\n",
    "    print(f\"    Stage {stage_idx+1}/{n_stages}: {sum(counts.values())} non-null entries.\")\n",
    "\n",
    "# 2. Identify insignificant receivers (below threshold, not 'unresponsive')\n",
    "print(\"2. Identifying insignificant receivers per stage...\")\n",
    "insignificant_receivers_by_stage = [\n",
    "    {rcv for rcv, cnt in receiver_counts.items() if cnt < min_count and rcv != 'unresponsive'}\n",
    "    for receiver_counts in stage_receiver_counts\n",
    "]\n",
    "for stage_idx, insign_set in enumerate(insignificant_receivers_by_stage):\n",
    "    print(f\"    Stage {stage_idx+1}: {len(insign_set)} insignificant receivers.\")\n",
    "\n",
    "def get_label(val, insignificant_set):\n",
    "    \"\"\"Map raw value to 'unresponsive', 'others', or stringified value.\"\"\"\n",
    "    if pd.isna(val):\n",
    "        return 'unresponsive'\n",
    "    if (val_str := str(val)) in insignificant_set:\n",
    "        return 'others'\n",
    "    return '-'.join(val_str.split('-')[:-1])\n",
    "\n",
    "# 3. Build node sets per stage, mapping 'others' and 'unresponsive'\n",
    "print(\"3. Building node sets per stage (with collapsed labels)...\")\n",
    "stage_nodes = []\n",
    "for stage_idx, receiver_col in enumerate(receiver_cols):\n",
    "    print(f\"    Stage {stage_idx+1}/{n_stages}...\")\n",
    "    vals = df_seq[receiver_col].values\n",
    "    node_set = set(get_label(v, insignificant_receivers_by_stage[stage_idx]) for v in vals)\n",
    "    stage_nodes.append(node_set)\n",
    "\n",
    "# 4. Fast: Count transitions using itertuples (by index)\n",
    "print(\"4. Counting transitions between stages (collapsed labels, fast tuple index)...\")\n",
    "transition_counts = [defaultdict(int) for _ in range(n_stages - 1)]\n",
    "col_idx = [df_seq.columns.get_loc(col) for col in receiver_cols]\n",
    "n_rows = df_seq.shape[0]\n",
    "log_every = max(1, n_rows // 10)\n",
    "for i, row in enumerate(df_seq.itertuples(index=False, name=None)):\n",
    "    if (i % log_every == 0) or (i == n_rows - 1):\n",
    "        print(f\"    Processed {i+1}/{n_rows} rows...\", end=\"\\r\" if i != n_rows-1 else \"\\n\", flush=True)\n",
    "    for stage in range(n_stages - 1):\n",
    "        src_val = row[col_idx[stage]]\n",
    "        dst_val = row[col_idx[stage + 1]]\n",
    "        src_label = get_label(src_val, insignificant_receivers_by_stage[stage])\n",
    "        dst_label = get_label(dst_val, insignificant_receivers_by_stage[stage+1])\n",
    "        transition_counts[stage][(src_label, dst_label)] += 1\n",
    "\n",
    "# 5. Assign node indices and build node list (unchanged except for logging)\n",
    "print(\"5. Assigning node indices and building node list...\")\n",
    "nodes = []\n",
    "node_to_index = {}\n",
    "for stage_idx, node_labels in enumerate(stage_nodes):\n",
    "    print(f\"    Stage {stage_idx+1}/{n_stages}: {len(node_labels)} nodes.\")\n",
    "    for label in sorted(node_labels):\n",
    "        node_to_index[(label, stage_idx)] = len(nodes)\n",
    "        nodes.append((label, stage_idx))\n",
    "\n",
    "# 6. Build Sankey links\n",
    "print(\"6. Building Sankey links...\")\n",
    "links = []\n",
    "for stage, stage_counts in enumerate(transition_counts):\n",
    "    print(f\"    Stage {stage+1}: {len(stage_counts)} transitions.\")\n",
    "    for (src, dst), count in stage_counts.items():\n",
    "        src_idx = node_to_index[(src, stage)]\n",
    "        dst_idx = node_to_index[(dst, stage+1)]\n",
    "        links.append((src_idx, dst_idx, count))\n",
    "\n",
    "# 7. Make node labels and values for Sankey\n",
    "print(\"7. Assigning node labels and values for Sankey diagram...\")\n",
    "node_labels = [lbl for lbl, _ in nodes]\n",
    "flow_in = defaultdict(int)\n",
    "flow_out = defaultdict(int)\n",
    "for src_idx, dst_idx, val in links:\n",
    "    flow_out[src_idx] += val\n",
    "    flow_in[dst_idx] += val\n",
    "node_values = [flow_out[i] if stage == 0 else flow_in[i] for i, (_, stage) in enumerate(nodes)]\n",
    "node_labels_with_values = [f\"{label}\\n({node_values[idx]})\" for idx, label in enumerate(node_labels)]\n",
    "\n",
    "# 8. Prepare Sankey lists\n",
    "print(\"8. Preparing Sankey diagram lists (sources, targets, values)...\")\n",
    "source_indices = [src for src, _, _ in links]\n",
    "target_indices = [dst for _, dst, _ in links]\n",
    "flow_values =    [val for _, _, val in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e735407e-10a9-40a3-a002-0425c1d5293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT AND SAVE\n",
    "\n",
    "# Assign each target / label one fixed color over all stages\n",
    "receiver_color_map = {\n",
    "    'es-mad':       '#FFA500',\n",
    "    'unresponsive': '#D62728',\n",
    "    'others':       '#CCCCCC'\n",
    "}\n",
    "palette = plc.qualitative.Plotly\n",
    "\n",
    "# Step 2: Ensure all receivers have a color\n",
    "all_receiver_labels = set(label for (label, stage) in nodes)\n",
    "for rcv in all_receiver_labels:\n",
    "    if rcv not in receiver_color_map:\n",
    "        receiver_color_map[rcv] = palette[hash(rcv) % len(palette)]\n",
    "\n",
    "# Step 3: Create list of colors for nodes and links\n",
    "node_colors = [receiver_color_map[label] for (label, stage) in nodes]\n",
    "link_colors = [receiver_color_map[nodes[src][0]] for src, dst, val in links]\n",
    "\n",
    "# Create and configure the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=node_labels_with_values,\n",
    "        color=node_colors\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source_indices,\n",
    "        target=target_indices,\n",
    "        value=flow_values,\n",
    "        #color=link_colors\n",
    "    )\n",
    ")])\n",
    "\n",
    "# Annotate each stage with its corresponding date\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        \"text\": f\"Receiver transitions from {FROM.strftime('%d. %B')} to {TO.strftime('%d. %B %Y')} (â‰¥{min_count})\",\n",
    "        \"x\": 0.5,\n",
    "        \"y\": 0.95,\n",
    "        \"xanchor\": \"center\",\n",
    "        \"yanchor\": \"top\",\n",
    "        \"font\": dict(size=22)\n",
    "    },\n",
    "    font_size=12,\n",
    "    annotations=[\n",
    "        dict(\n",
    "            x=i / (len(labels) - 1),\n",
    "            y=1.04,\n",
    "            text=f\"<b>{label}</b>\",\n",
    "            showarrow=False,\n",
    "            font=dict(size=16)\n",
    "        )\n",
    "        for i, label in enumerate(labels)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Export the interactive diagram to an HTML file\n",
    "fig.write_html(f\"sankey-{labels[0]}-{labels[-1]}-h{min_count}-v{IP_VERSION}.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7795487d-5204-47d7-8e9e-8971a5936891",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "import json\n",
    "\n",
    "per_day = defaultdict(list)\n",
    "for i in range(len(source_indices)):\n",
    "    # Get the indices\n",
    "    from_idx = source_indices[i]\n",
    "    to_idx = target_indices[i]\n",
    "    # Get the data\n",
    "    from_receiver, from_stage = nodes[from_idx]\n",
    "    to_receiver, to_stage = nodes[to_idx]\n",
    "    \n",
    "    # Build the per day entry\n",
    "    per_day[from_stage].append({\n",
    "        \"from\": from_receiver,\n",
    "        \"to\": to_receiver,\n",
    "        \"value\": flow_values[i]\n",
    "    })\n",
    "\n",
    "# Write to file\n",
    "with open(f\"{labels[0]}-{labels[-1]}-h{min_count}-v{IP_VERSION}.json\", \"w\") as f:\n",
    "    json.dump(dict(per_day), f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
