{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bae57b8e23c77b0",
   "metadata": {},
   "source": [
    "# Iberian outage analysis\n",
    "\n",
    "We have access to daily data for 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888717f3fe793105",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:07:16.983752Z",
     "start_time": "2025-05-16T08:07:16.926682Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gzip\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import ipaddress\n",
    "\n",
    "import boto3\n",
    "from botocore.utils import fix_s3_host\n",
    "from botocore.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20612408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load secret environment variables\n",
    "with open(\".env\") as f:\n",
    "    for line in f:\n",
    "        if line.strip() == '' or line.strip().startswith('#'):\n",
    "            continue\n",
    "        key, value = line.strip().split('=', 1)\n",
    "        os.environ[key] = value\n",
    "\n",
    "# Set up global constants\n",
    "DATA_DIR = \"/data\"\n",
    "ROWS = 1_000_000\n",
    "\n",
    "# The date of the data we are using\n",
    "YEAR = 2025\n",
    "MONTH = 5\n",
    "DAY = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6944c2836231af8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:07:17.901498Z",
     "start_time": "2025-05-16T08:07:17.659357Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create boto3 resource using environment variables\n",
    "S3 = boto3.resource(\n",
    "    's3',\n",
    "    aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
    "    aws_secret_access_key=os.environ['AWS_ACCESS_KEY_SECRET'],\n",
    "    endpoint_url=os.environ['AWS_ENDPOINT_URL'],\n",
    "    # Change timeouts in case we are uploading large files\n",
    "    config=Config(\n",
    "        connect_timeout=3, \n",
    "        read_timeout=900, \n",
    "        retries={\"max_attempts\":0}\n",
    "    )\n",
    ")\n",
    "\n",
    "# Unregister to ensure requests donâ€™t go to AWS\n",
    "S3.meta.client.meta.events.unregister('before-sign.s3', fix_s3_host)\n",
    "\n",
    "# Use bucket name from environment\n",
    "HOME_BUCKET = S3.Bucket(os.environ['AWS_BUCKET_NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3886512a74ee2cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:07:21.570989Z",
     "start_time": "2025-05-16T08:07:21.564307Z"
    }
   },
   "outputs": [],
   "source": [
    "# MAnycastR files have metadata that give information about the measurement\n",
    "def read_gzipped_comment_lines(filepath, comment_char='#'):\n",
    "    \"\"\"Read initial comment lines from a gzipped file.\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File not found: '{filepath}'\")\n",
    "\n",
    "    try:\n",
    "        with gzip.open(filepath, 'rt', encoding='utf-8') as f:\n",
    "            comment_lines = []\n",
    "            # Read lines until the first non-comment line\n",
    "            for line in f:\n",
    "                if line.startswith(comment_char):\n",
    "                    comment_lines.append(line.rstrip())\n",
    "                else:\n",
    "                    # Stop at first non-comment line\n",
    "                    break\n",
    "            return comment_lines\n",
    "\n",
    "    except gzip.BadGzipFile:\n",
    "        raise gzip.BadGzipFile(f\"Invalid gzip file: '{filepath}'\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error reading file '{filepath}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4dcec090dc5467",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:07:22.226791Z",
     "start_time": "2025-05-16T08:07:22.220080Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to create a hostname mapping (Client ID -> hostname)\n",
    "def create_hostname_mapping(comment_lines):\n",
    "    \"\"\"Map Client ID to hostname from comment lines.\"\"\"\n",
    "    pattern = r\"ID:\\s*(\\d+)\\s*,\\s*hostname:\\s*([\\w-]+)\"\n",
    "    mapping = {}\n",
    "\n",
    "    for line in comment_lines:\n",
    "        if (match := re.search(pattern, line)):\n",
    "            client_id = int(match.group(1))       # Extract Client ID\n",
    "            hostname = match.group(2)             # Extract hostname\n",
    "            mapping[client_id] = hostname\n",
    "\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6826fbce3302ed2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:16:27.145564Z",
     "start_time": "2025-05-16T08:16:27.139848Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_manycast_file(year, month, day, anycast=True, ipv6=False):\n",
    "    \"\"\"Download a (Manycast/Unicast, IPv4/IPv6) file from MinIO if not already present.\"\"\"\n",
    "\n",
    "    # Build object prefix based on date\n",
    "    prefix = f\"manycast/{year}/{month:02}/{day:02}/\"\n",
    "\n",
    "    # Choose file pattern based on anycast and IP version\n",
    "    protocol = \"ICMPv6\" if ipv6 else \"ICMPv4\"\n",
    "    base_pattern = f\"MAnycast_{protocol}\" if anycast else f\"GCD_{protocol}\"\n",
    "\n",
    "    # Search for matching file in bucket\n",
    "    for obj in HOME_BUCKET.objects.filter(Prefix=prefix):\n",
    "         # Replace invalid Windows characters in filenames\n",
    "        filename = re.sub(r'[:<>\"/\\\\|?*]', '_', obj.key[len(prefix):])\n",
    "        filepath = os.path.join(DATA_DIR, filename)\n",
    "\n",
    "        if filename.startswith(base_pattern) and filename.endswith('.csv.gz'):\n",
    "            print(f\"Found file: {filename} (bucket key: {obj.key})\")\n",
    "\n",
    "            # Check if file already exists locally\n",
    "            if os.path.exists(filepath):\n",
    "                print(f\"File {filename} already exists. Skipping download.\")\n",
    "            else:\n",
    "                print(f\"Downloading {filename} from bucket...\")\n",
    "                os.makedirs(DATA_DIR, exist_ok=True)\n",
    "                HOME_BUCKET.download_file(obj.key, filepath)\n",
    "\n",
    "            return filepath\n",
    "\n",
    "    print(\"No matching file found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca152d2da13105fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:19:19.853107Z",
     "start_time": "2025-05-16T08:16:31.949823Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download the file for the specified date\n",
    "filepath = get_manycast_file(YEAR, MONTH, DAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d701f00e5baedea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:19:25.702473Z",
     "start_time": "2025-05-16T08:19:25.695681Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get metadata\n",
    "comment_lines = read_gzipped_comment_lines(filepath)\n",
    "print(\"\\n\".join(comment_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd45ef10e10863a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:19:30.326275Z",
     "start_time": "2025-05-16T08:19:30.319848Z"
    }
   },
   "outputs": [],
   "source": [
    "# create hostname mapping\n",
    "hostname_map = create_hostname_mapping(comment_lines)\n",
    "hostname_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bfee5566d09679",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:22:02.163186Z",
     "start_time": "2025-05-16T08:20:54.980130Z"
    }
   },
   "outputs": [],
   "source": [
    "# load in data as a pandas dataframe\n",
    "result_df = pd.read_csv(filepath, skiprows=len(comment_lines), nrows=ROWS, compression='gzip')\n",
    "result_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21080d3f5cd6ef26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T08:23:23.517919Z",
     "start_time": "2025-05-16T08:23:20.551124Z"
    }
   },
   "outputs": [],
   "source": [
    "# data cleaning as we are only using the following columns:\n",
    "result_df = result_df[['rx_worker_id', 'tx_worker_id', 'reply_src_addr', 'rx_time', 'tx_time', 'ttl']]\n",
    "result_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaf55f08b02b0fd",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-16T08:23:45.720902Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# we scan an address in each /24 prefix, which is representative of that prefix\n",
    "# each target is scanned by all 32 anycast sites\n",
    "# in your case this might give redundancy, so make sure to take into consideration that you will see the same prefix multiple times (e.g., remove duplicate targets from the analysis)\n",
    "\n",
    "# convert IP-number to ip network\n",
    "result_df['target'] = result_df['reply_src_addr'].apply(\n",
    "    # can be sped up with swifter\n",
    "    lambda x: ipaddress.ip_network(f\"{ipaddress.ip_address(x)}/24\", strict=False)\n",
    ")\n",
    "\n",
    "# get receiving anycast site\n",
    "result_df['receiver'] = result_df['rx_worker_id'].map(hostname_map)\n",
    "\n",
    "# get sending anycast site\n",
    "result_df['sender'] = result_df['tx_worker_id'].map(hostname_map)\n",
    "\n",
    "# calculate rtt\n",
    "result_df['rtt'] = ((result_df['rx_time'] - result_df['tx_time']) / 1e6)\n",
    "\n",
    "# drop unnecessary columns\n",
    "result_df = result_df[['receiver', 'sender', 'target', 'rtt', 'ttl']]\n",
    "\n",
    "result_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c2d4837c5acfe6",
   "metadata": {},
   "source": [
    "**Some ideas:**\n",
    "- what is the impact on average hop count (TTL)?\n",
    "- what is the impact on average RTT?\n",
    "- which prefixes became unreachable?\n",
    "- which prefixes shifted catchment?\n",
    "- where did the prefixes that switched catchment go?\n",
    "\n",
    "Also consider looking at GCD and filtering on sender == madrid -> how many prefixes are reachable from there?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
